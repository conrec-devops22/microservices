# Microservices Basic Lab

Mission: Make two microservices, make them communicate privately and open up one for host access (http://localhost:5002)

Certainly! Here's the updated README with the latest changes, and in Step 1, I'll explain only the statements related to logging:

# Adding Fluentd, Elasticsearch, and Kibana to out microservices

In this guide, we will go through the process of integrating Fluentd, Elasticsearch, and Kibana with an existing Python application using Docker Compose. This setup will allow you to centralize and visualize logs generated by our Python app while using a custom Fluentd Dockerfile.

## Prerequisites

Before you begin, ensure you have the following prerequisites installed on your system:

- Docker: https://docs.docker.com/get-docker/
- Docker Compose: https://docs.docker.com/compose/install/
- The microservices ready in containers.

## Step 1: Prepare the microservices for logging

We are going to use the internal logging library from Python:

```python
import json
import sys
import logging
from flask import Flask, request
from prometheus_flask_exporter import PrometheusMetrics

class StructuredFormatter(logging.Formatter):

    def format(self, record):
        log_entry = {
            "level": record.levelname,
            "message": record.getMessage(),
            "timestamp": self.formatTime(record),
        }
        if request:
            log_entry.update(
                {
                    "path": request.path,
                    "method": request.method,
                    "remote_addr": request.remote_addr,
                }
            )
        if record.exc_info:
            log_entry["exception"] = self.formatException(record.exc_info)
        return json.dumps(log_entry)

app = Flask(__name__)

# Set up our logging handler to use StructuredFormatter
handler = logging.StreamHandler(sys.stdout)
handler.setFormatter(StructuredFormatter())

# Add the handler to the Flask app's logger
app.logger.addHandler(handler)
app.logger.setLevel(logging.INFO)

# Also change the werkzeug logger, the default Flask logger, to use StructuredFormatter
logging.getLogger("werkzeug").addHandler(handler)
```

- `json`: This module is used for working with JSON data.
- `sys`: This module provides access to some variables used or maintained by the interpreter and to functions that interact with the interpreter.
- `logging`: The Python logging library allows you to configure and emit log messages from your application.
- `request`: It's an object provided by Flask to represent the incoming HTTP request.

The code defines a custom logging formatter (`StructuredFormatter`) that structures log entries as JSON objects and adds request context if available. It also configures the Flask app's logger and the Werkzeug logger to use this formatter.

## Step 2: Update the Docker Compose File

Add the following images to our `docker-compose.yaml` file.
### `fluentd` Service

```yaml
fluentd:
  build: ./fluentd
  container_name: fluentd-container
  volumes:
    - ./fluentd/conf:/fluentd/etc
  networks:
    - "my_network"
  ports:
    - "24224:24224"
    - "24224:24224/udp"
  depends_on:
    - elasticsearch
```

- `build: ./fluentd`: This specifies that the `fluentd` service should be built using a Dockerfile located in the `./fluentd` directory. The `Dockerfile` in that directory defines how the Fluentd container should be created.
- `container_name: fluentd-container`: This sets a custom name for the container. Containers are typically named automatically, but this option allows you to provide a specific name.
- `volumes`: This section defines a volume mapping between the host machine and the container. It maps the `./fluentd/conf` directory on the host to `/fluentd/etc` inside the container. This is used for configuration data.
- `networks`: It assigns the service to a Docker network called `"my_network"`. This allows containers within the same network to communicate with each other using their service names.
- `ports`: This maps ports between the host and the container. Fluentd is configured to listen on port `24224` for both TCP and UDP traffic. This allows logs to be received by Fluentd from other services.
- `depends_on`: This specifies that the `fluentd` service depends on the `elasticsearch` service. Docker Compose will ensure that `elasticsearch` starts before `fluentd`.
### `elasticsearch` Service

```yaml
elasticsearch:
  image: docker.elastic.co/elasticsearch/elasticsearch:8.9.2
  container_name: elasticsearch
  environment:
    - "discovery.type=single-node"
    - "xpack.security.enabled=false"
    - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
  networks:
    - "my_network"
  ports:
    - "9200:9200"
  ulimits:
    memlock:
      soft: -1
      hard: -1
    nofile:
      soft: 65535
      hard: 65535
  cap_add:
    - IPC_LOCK
  volumes:
    - elasticsearch-data-volume:/usr/share/elasticsearch/data
```

- `image`: This specifies the Docker image to use for the `elasticsearch` service. It uses the official Elasticsearch image from Docker Hub with version `8.9.2`.
- `container_name`: This sets a custom name for the Elasticsearch container.
- `environment`: Here, several environment variables are defined. These variables configure Elasticsearch:
  - `discovery.type=single-node`: Configures Elasticsearch to run as a single-node cluster.
  - `xpack.security.enabled=false`: Disables security features.
  - `ES_JAVA_OPTS=-Xms512m -Xmx512m`: Sets JVM memory options for Elasticsearch.
- `networks`: Similar to the `fluentd` service, it assigns the `elasticsearch` service to the `"my_network"` Docker network.
- `ports`: Maps port `9200` from the host to port `9200` in the Elasticsearch container. This allows you to access Elasticsearch's REST API.
- `ulimits`: Specifies resource limits for the container. It ensures that Elasticsearch can use sufficient memory and file descriptors.
- `cap_add`: Adds Linux capabilities to the container. In this case, `IPC_LOCK` capability is added, which is used by Elasticsearch for memory locking.
- `volumes`: Creates a Docker volume named `elasticsearch-data-volume` and mounts it to the `/usr/share/elasticsearch/data` directory inside the container. This is where Elasticsearch stores its data.
### `kibana` Service

```yaml
kibana:
  image: docker.elastic.co/kibana/kibana:8.9.2
  networks:
    - "my_network"
  ports:
    - "5601:5601"
  environment:
    - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
  depends_on:
    - elasticsearch
```

- `image`: Specifies the Docker image for the `kibana` service. It uses the official Kibana image from Docker Hub with version `8.9.2`.
- `networks`: Assigns the service to the `"my_network"` Docker network.
- `ports`: Maps port `5601` from the host to port `5601` in the Kibana container, allowing you to access the Kibana web interface.
- `environment`: Sets the `ELASTICSEARCH_HOSTS` environment variable to `http://elasticsearch:9200`, which tells Kibana where to find Elasticsearch.
- `depends_on`: Specifies that the `kibana` service depends on the `elasticsearch` service, ensuring that `elasticsearch` starts before `kibana`.

## Step 4: Configure Fluentd

Create a Fluentd configuration file (`fluentd.conf`) inside a `fluentd/conf` directory, which you mounted in the `fluentd` service in the `docker-compose.yml` file. Customize the configuration to match your log format and desired outputs.

Let's break down the configuration in the `fluent.conf` file:

```conf
<source>
  @type forward
  port 24224
  bind 0.0.0.0
</source>
```

- `<source>`: This block defines a Fluentd input source. In this case, it specifies that Fluentd should listen for log entries from a source using the Forward protocol. The `@type` directive sets the source type to "forward."
- `port 24224`: Specifies that Fluentd should listen on port 24224 for incoming log entries.
- `bind 0.0.0.0`: Binds Fluentd to all available network interfaces, making it accessible from any IP address.

```conf
<filter service1.**>
  @type parser
  key_name log
  reserve_data true
  <parse>
    @type multi_format
    <pattern>
      format json
      time_key time
      time_format %Y-%m-%dT%H:%M:%S.%NZ
    </pattern>
    <pattern>
      format /^(?<log>.*)$/
    </pattern>
  </parse>
</filter>
```

- `<filter service1.**>`: This block defines a filter for log entries matching the tag pattern `service1.**`. Filters are used to process and transform log entries before forwarding them to their destinations.
- `@type parser`: Specifies that Fluentd should use a parser to process log entries.
- `key_name log`: Sets the key name within log entries that the parser should operate on.
- `reserve_data true`: Ensures that the original log data is preserved even after parsing.
- `<parse>`: This block defines multiple log parsing patterns using the `multi_format` parser. It attempts to parse log entries using different formats:
  - The first `<pattern>` block specifies that log entries should be parsed as JSON and extracts the timestamp (`time_key`) using the specified time format.
  - The second `<pattern>` block is a fallback that captures any log entry that doesn't match the JSON format.

```conf
<match **>
  @type copy

  <store>
    @type elasticsearch
    host elasticsearch
    port 9200
    logstash_format true
    logstash_prefix fluentd
    logstash_dateformat %Y%m%d
    include_tag_key true
    type_name access_log
    tag_key @log_name
    flush_interval 1s
  </store>

  <store>
    @type stdout
  </store>
</match>
```

- `<match **>`: This block defines a Fluentd match pattern to determine how log entries should be processed and sent to various destinations.
- `@type copy`: Specifies that Fluentd should make a copy of the log entries for further processing and forwarding.
- `<store>`: Inside the `<match **>` block, there are two destinations defined:
  - The first `<store>` block sends log entries to Elasticsearch. It specifies the Elasticsearch host, port, and various formatting options, such as using the Logstash format and defining a date format.
  - The second `<store>` block sends log entries to the standard output (stdout), which is typically used for debugging and development purposes.

## Step 5: Start the Services

Start all the services defined in the `docker-compose.yml` file:

```bash
docker-compose up
```

## Step 6: Access Kibana

Access Kibana in your web browser by navigating to `http://localhost:5601`. You can now configure Kibana to visualize and explore the logs stored in Elasticsearch.

## Step 7: Verify Logs

Ensure that your Python application's logs are being collected by Fluentd and stored in Elasticsearch. You can use Kibana's interface to search and visualize your logs.
